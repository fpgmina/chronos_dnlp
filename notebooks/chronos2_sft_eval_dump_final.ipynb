{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d308bb2e",
      "metadata": {
        "id": "d308bb2e"
      },
      "source": [
        "# Chronos-2 SFT — Evaluation dumps for statistical testing (Global + Industry)\n",
        "\n",
        "This notebook **does not perform training** and does not alter the SFT training notebook.\n",
        "It assumes that the SFT checkpoints have already been produced by the training notebook and are available under:\n",
        "`outputs/chronos2_sft/<group>/finetuned-ckpt` (relative to the `notebooks/` directory).\n",
        "\n",
        "The goal is to **re-run the same evaluation procedure** used in the project codebase, but additionally **save per-ticker, per-window results** to disk.\n",
        "These dumps are then used by a separate notebook to compute paired statistical tests (e.g., Wilcoxon + bootstrap CIs).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7c3b2973",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 22,
          "status": "ok",
          "timestamp": 1770901644092,
          "user": {
            "displayName": "Lorenzo Ferrandi",
            "userId": "04697785781964806390"
          },
          "user_tz": -60
        },
        "id": "7c3b2973",
        "outputId": "02cc92aa-94e8-42c5-e713-84801356c254"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEVICE: cuda\n",
            "GPU: NVIDIA GeForce RTX 4060 Laptop GPU capability: (8, 9)\n",
            "torch: 2.9.1\n"
          ]
        }
      ],
      "source": [
        "# Project imports and environment setup\n",
        "#\n",
        "# Expected usage: run this notebook from the repository's `notebooks/` directory.\n",
        "# The training notebook writes checkpoints under `outputs/chronos2_sft/...` relative to that directory.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "current_dir = os.getcwd()\n",
        "REPO_ROOT = os.path.dirname(current_dir) # Using this notation to keep the rest of the code the same.\n",
        "sys.path.append(REPO_ROOT)\n",
        "\n",
        "\n",
        "from tiingo_data.download_data import get_daily_returns_data_cached\n",
        "from core.data import prepare_data_for_chronos, GICS_LEVEL_1\n",
        "from utils import get_device\n",
        "from chronos import Chronos2Pipeline\n",
        "\n",
        "DEVICE = get_device()\n",
        "print(\"DEVICE:\", DEVICE)\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0), \"capability:\", torch.cuda.get_device_capability(0))\n",
        "print(\"torch:\", torch.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f37140ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 496,
          "status": "ok",
          "timestamp": 1770901647509,
          "user": {
            "displayName": "Lorenzo Ferrandi",
            "userId": "04697785781964806390"
          },
          "user_tz": -60
        },
        "id": "f37140ef",
        "outputId": "f8c1f344-4493-4f41-b005-f763c3dd50cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "df_train_clean: (2797, 114) df_eval_clean: (1200, 114)\n",
            "eval date range: 2021-02-12 00:00:00+00:00 → 2025-11-20 00:00:00+00:00\n"
          ]
        }
      ],
      "source": [
        "# Load daily returns and build the exact train/eval split used for training/evaluation\n",
        "df_all = get_daily_returns_data_cached()\n",
        "df_train_clean, df_eval_clean = prepare_data_for_chronos(df_all, test_size=1200)\n",
        "\n",
        "print(\"df_train_clean:\", df_train_clean.shape, \"df_eval_clean:\", df_eval_clean.shape)\n",
        "print(\"eval date range:\", df_eval_clean.index[0], \"→\", df_eval_clean.index[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "14e55289",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 18,
          "status": "ok",
          "timestamp": 1770901649017,
          "user": {
            "displayName": "Lorenzo Ferrandi",
            "userId": "04697785781964806390"
          },
          "user_tz": -60
        },
        "id": "14e55289",
        "outputId": "14005808-8fc9-49ad-fb8c-c19520db6c38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GLOBAL HP: {'prediction_length': 1, 'context_length': 128, 'num_steps': 1500, 'batch_size': 48, 'learning_rate': 3.113813151474403e-06, 'stride': 100, 'n_eval_samples': 100}\n"
          ]
        }
      ],
      "source": [
        "# Load the best hyperparameters found during the Optuna tuning stage.\n",
        "# The CSV is produced by the training notebook and stored under outputs/.\n",
        "CSV_CANDIDATES = [\n",
        "    Path(REPO_ROOT) / \"notebooks\" / \"outputs\" / \"tuning_results\" / \"tuning_best_results.csv\",\n",
        "    Path(REPO_ROOT) / \"outputs\" / \"tuning_results\" / \"tuning_best_results.csv\"\n",
        "]\n",
        "best_csv_path = next((p for p in CSV_CANDIDATES if p.exists()), None)\n",
        "if best_csv_path is None:\n",
        "    raise FileNotFoundError(\n",
        "        \"tuning_best_results.csv not found. Searched:\\n\" + \"\\n\".join(map(str, CSV_CANDIDATES))\n",
        "    )\n",
        "\n",
        "df_best = pd.read_csv(best_csv_path)\n",
        "df_best[\"group\"] = df_best[\"group\"].astype(str).str.strip()\n",
        "hp_by_group = df_best.set_index(\"group\").to_dict(orient=\"index\")\n",
        "\n",
        "# Forecast horizon used throughout the project (one-step ahead)\n",
        "PREDICTION_LENGTH = 1\n",
        "\n",
        "\n",
        "def get_hparams(group: str):\n",
        "    \"\"\"Return the tuned hyperparameters for a given evaluation group.\"\"\"\n",
        "    g = str(group).strip()\n",
        "    if g not in hp_by_group:\n",
        "        raise KeyError(f\"Group '{g}' not present in the tuning CSV. Available: {sorted(hp_by_group.keys())}\")\n",
        "    row = hp_by_group[g]\n",
        "    return dict(\n",
        "        prediction_length=PREDICTION_LENGTH,\n",
        "        context_length=int(row[\"context_length\"]),\n",
        "        num_steps=int(row[\"num_steps\"]),\n",
        "        batch_size=int(row[\"batch_size\"]),\n",
        "        learning_rate=float(row[\"learning_rate\"]),\n",
        "        stride=int(row.get(\"stride\", 50)),\n",
        "        n_eval_samples=int(row.get(\"n_eval_samples\", 100)),\n",
        "    )\n",
        "\n",
        "\n",
        "global_hp = get_hparams(\"global\")\n",
        "print(\"GLOBAL HP:\", global_hp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b396ef01",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 10,
          "status": "ok",
          "timestamp": 1770901651541,
          "user": {
            "displayName": "Lorenzo Ferrandi",
            "userId": "04697785781964806390"
          },
          "user_tz": -60
        },
        "id": "b396ef01",
        "outputId": "59687dfa-60fd-4287-c2be-8a9feb85fa92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: global SFT checkpoint found: outputs\\chronos2_sft\\general\\finetuned-ckpt\n"
          ]
        }
      ],
      "source": [
        "# Utility: normalize group names to directory-friendly slugs (matches the training notebook)\n",
        "def slugify(category: str) -> str:\n",
        "    return (\n",
        "        category.lower()\n",
        "        .replace(\"&\", \"and\")\n",
        "        .replace(\"/\", \"_\")\n",
        "        .replace(\" \", \"_\")\n",
        "    )\n",
        "\n",
        "# Checkpoint/output base directory.\n",
        "# The training notebook uses output_dir=\"outputs/...\"; when run from `notebooks/`, this resolves to `notebooks/outputs/...`.\n",
        "OUTPUTS_BASE = Path(\"outputs\")\n",
        "\n",
        "# Expected location of the global fine-tuned checkpoint\n",
        "general_ckpt = OUTPUTS_BASE / \"chronos2_sft\" / \"general\" / \"finetuned-ckpt\"\n",
        "if not general_ckpt.exists():\n",
        "    print(\"WARNING: global SFT checkpoint not found at:\", general_ckpt)\n",
        "    print(\"Make sure you have run the training notebook and that outputs/ is in the expected location.\")\n",
        "else:\n",
        "    print(\"OK: global SFT checkpoint found:\", general_ckpt)\n",
        "\n",
        "\n",
        "def category_ckpt(category: str) -> Path:\n",
        "    \"\"\"Return the fine-tuned checkpoint path for an industry/category group.\"\"\"\n",
        "    return OUTPUTS_BASE / \"chronos2_sft\" / slugify(category) / \"finetuned-ckpt\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f160f0a1",
      "metadata": {
        "executionInfo": {
          "elapsed": 18,
          "status": "ok",
          "timestamp": 1770901653604,
          "user": {
            "displayName": "Lorenzo Ferrandi",
            "userId": "04697785781964806390"
          },
          "user_tz": -60
        },
        "id": "f160f0a1"
      },
      "outputs": [],
      "source": [
        "# Metric helpers\n",
        "#\n",
        "# We compute per-ticker MAE/MSE using the median forecast and per-ticker MQL using the pinball loss\n",
        "# averaged over the quantiles 0.1..0.9.\n",
        "\n",
        "QUANTILES = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], dtype=np.float32)\n",
        "\n",
        "\n",
        "def per_ticker_mql(y_true: np.ndarray, y_pred_quantiles: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Mean Quantile Loss (pinball loss), returned per ticker.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true: (N,) array\n",
        "        True 1-step-ahead value for each ticker.\n",
        "    y_pred_quantiles: (N, Q) array\n",
        "        Predicted quantiles for each ticker. If Q > 9, we keep the first 9 columns\n",
        "        corresponding to the 0.1..0.9 grid used in the project.\n",
        "    \"\"\"\n",
        "    y_pred_9 = y_pred_quantiles[:, : len(QUANTILES)]  # (N, 9)\n",
        "    errors = y_true[:, None] - y_pred_9              # (N, 9)\n",
        "    q = QUANTILES[None, :]                           # (1, 9)\n",
        "    pin = np.where(errors >= 0, q * errors, (q - 1.0) * errors)\n",
        "    return pin.mean(axis=1)\n",
        "\n",
        "\n",
        "def sample_start_indices(T: int, context_length: int, n_samples: int, seed: int = 42) -> np.ndarray:\n",
        "    \"\"\"Sample evaluation window start indices.\n",
        "\n",
        "    This mirrors the evaluation procedure used in the project code (`core/eval.py`):\n",
        "    we set the NumPy RNG seed and sample start indices without replacement.\n",
        "    \"\"\"\n",
        "    max_start = T - context_length - 1\n",
        "    if max_start <= 0:\n",
        "        raise ValueError(f\"Series too short: T={T}, context_length={context_length}\")\n",
        "    np.random.seed(seed)\n",
        "    n = min(int(n_samples), int(max_start))\n",
        "    return np.random.choice(max_start, size=n, replace=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fb3dfdef",
      "metadata": {
        "executionInfo": {
          "elapsed": 7,
          "status": "ok",
          "timestamp": 1770901655988,
          "user": {
            "displayName": "Lorenzo Ferrandi",
            "userId": "04697785781964806390"
          },
          "user_tz": -60
        },
        "id": "fb3dfdef"
      },
      "outputs": [],
      "source": [
        "# Core routine: evaluate a model on sampled windows and dump per-ticker results to disk\n",
        "def eval_dump_windows(\n",
        "    pipeline: Chronos2Pipeline,\n",
        "    df_test: pd.DataFrame,\n",
        "    context_length: int,\n",
        "    start_indices: np.ndarray,\n",
        "    model_name: str,\n",
        "    group_name: str,\n",
        "    out_path: Path,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Run 1-step-ahead evaluation on a set of sampled windows and save results.\n",
        "\n",
        "    For each sampled start index we build a multivariate context window of shape (N, context_length),\n",
        "    predict the next step, and store per-ticker metrics (MAE/MSE/MQL) plus the 0.1..0.9 quantile forecasts.\n",
        "    \"\"\"\n",
        "    data = df_test.values.astype(np.float32)  # (T, N)\n",
        "    T, N = data.shape\n",
        "    tickers = list(df_test.columns)\n",
        "    dates = df_test.index\n",
        "\n",
        "    rows = []\n",
        "    for window_id, start in enumerate(tqdm(start_indices, desc=f\"{model_name} | {group_name}\")):\n",
        "        # Build context and target for a 1-step-ahead forecast\n",
        "        ctx = data[start : start + context_length, :].T  # (N, context_length)\n",
        "        y_true = data[start + context_length, :]         # (N,)\n",
        "        date = dates[start + context_length]\n",
        "\n",
        "        # Predict quantiles for the next step\n",
        "        forecast = pipeline.predict([{\"target\": ctx}], prediction_length=1)\n",
        "        y_pred_q = forecast[0][:, :, 0].detach().cpu().numpy().astype(np.float32)  # (N, Q)\n",
        "        y_med = y_pred_q[:, 4]  # median index for the 0.1..0.9 grid\n",
        "\n",
        "        # Per-ticker point metrics\n",
        "        mae_t = np.abs(y_true - y_med)\n",
        "        mse_t = (y_true - y_med) ** 2\n",
        "\n",
        "        # Per-ticker quantile metric\n",
        "        mql_t = per_ticker_mql(y_true, y_pred_q)\n",
        "\n",
        "        # Emit one row per ticker for this window\n",
        "        for i, tkr in enumerate(tickers):\n",
        "            rows.append(\n",
        "                {\n",
        "                    \"window_id\": int(window_id),\n",
        "                    \"start_idx\": int(start),\n",
        "                    \"date\": date,\n",
        "                    \"ticker\": tkr,\n",
        "                    \"group\": group_name,\n",
        "                    \"model\": model_name,\n",
        "                    \"context_length\": int(context_length),\n",
        "                    \"y_true\": float(y_true[i]),\n",
        "                    \"y_pred_q10\": float(y_pred_q[i, 0]),\n",
        "                    \"y_pred_q20\": float(y_pred_q[i, 1]),\n",
        "                    \"y_pred_q30\": float(y_pred_q[i, 2]),\n",
        "                    \"y_pred_q40\": float(y_pred_q[i, 3]),\n",
        "                    \"y_pred_q50\": float(y_pred_q[i, 4]),\n",
        "                    \"y_pred_q60\": float(y_pred_q[i, 5]),\n",
        "                    \"y_pred_q70\": float(y_pred_q[i, 6]),\n",
        "                    \"y_pred_q80\": float(y_pred_q[i, 7]),\n",
        "                    \"y_pred_q90\": float(y_pred_q[i, 8]),\n",
        "                    \"mae\": float(mae_t[i]),\n",
        "                    \"mse\": float(mse_t[i]),\n",
        "                    \"mql\": float(mql_t[i]),\n",
        "                }\n",
        "            )\n",
        "\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    df_out = pd.DataFrame(rows)\n",
        "    df_out.to_parquet(out_path, index=False)\n",
        "    print(f\"Saved: {out_path} | rows={len(df_out)}\")\n",
        "    return df_out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c87561af",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 2270,
          "status": "ok",
          "timestamp": 1770901661045,
          "user": {
            "displayName": "Lorenzo Ferrandi",
            "userId": "04697785781964806390"
          },
          "user_tz": -60
        },
        "id": "c87561af",
        "outputId": "80e06e8a-d662-481b-f82d-49ff27c38b48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded models: baseline + global SFT\n"
          ]
        }
      ],
      "source": [
        "# Load the models to evaluate\n",
        "#\n",
        "# 1) Baseline: Chronos-2 zero-shot\n",
        "baseline = Chronos2Pipeline.from_pretrained(\n",
        "    \"amazon/chronos-2\",\n",
        "    device_map=DEVICE,\n",
        "    dtype=torch.float32,\n",
        ")\n",
        "\n",
        "# 2) Global SFT model: checkpoint produced by the training notebook\n",
        "if not general_ckpt.exists():\n",
        "    raise FileNotFoundError(f\"Global SFT checkpoint not found: {general_ckpt}\")\n",
        "\n",
        "sft_general = Chronos2Pipeline.from_pretrained(\n",
        "    str(general_ckpt),\n",
        "    device_map=DEVICE,\n",
        "    dtype=torch.float32,\n",
        ")\n",
        "\n",
        "print(\"Loaded models: baseline + global SFT\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "69b363a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 4244,
          "status": "ok",
          "timestamp": 1770901671632,
          "user": {
            "displayName": "Lorenzo Ferrandi",
            "userId": "04697785781964806390"
          },
          "user_tz": -60
        },
        "id": "69b363a0",
        "outputId": "37bbf10f-bfaf-4f40-807f-6976e3e9edd5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "baseline | global: 100%|██████████| 100/100 [00:05<00:00, 16.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\global__baseline.parquet | rows=11400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_general | global: 100%|██████████| 100/100 [00:05<00:00, 18.69it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\global__sft_general.parquet | rows=11400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# GLOBAL evaluation dumps (baseline vs global SFT)\n",
        "DUMPS_DIR = OUTPUTS_BASE / \"eval_dumps\" / \"sft\"\n",
        "DUMPS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "ctx_g = global_hp[\"context_length\"]\n",
        "n_g = global_hp[\"n_eval_samples\"]\n",
        "starts_g = sample_start_indices(len(df_eval_clean), ctx_g, n_g, seed=42)\n",
        "\n",
        "global_baseline_path = DUMPS_DIR / \"global__baseline.parquet\"\n",
        "global_sftgen_path = DUMPS_DIR / \"global__sft_general.parquet\"\n",
        "\n",
        "_ = eval_dump_windows(baseline, df_eval_clean, ctx_g, starts_g, \"baseline\", \"global\", global_baseline_path)\n",
        "_ = eval_dump_windows(sft_general, df_eval_clean, ctx_g, starts_g, \"sft_general\", \"global\", global_sftgen_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "Y52V8a-GvIIQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 4530,
          "status": "ok",
          "timestamp": 1770901681888,
          "user": {
            "displayName": "Lorenzo Ferrandi",
            "userId": "04697785781964806390"
          },
          "user_tz": -60
        },
        "id": "Y52V8a-GvIIQ",
        "outputId": "e79040bc-aa38-435a-88cb-f159b25f0579"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Sanity check: GLOBAL ---\n",
            "baseline (core): MQL= 0.0073292092  MAE= 0.016542953\n",
            "baseline (dump): MQL= 0.007329209622847358  MAE= 0.01654295417169985\n",
            "sft_general (core): MQL= 0.0058949403  MAE= 0.013767662\n",
            "sft_general (dump): MQL= 0.005894940915168263  MAE= 0.013767661353317395\n",
            "Abs. diff baseline (dump-core): 3.8083128057336824e-10 1.663965552151092e-09\n",
            "Abs. diff sft_general (dump-core): 5.686888468817153e-10 1.1048043816602737e-09\n"
          ]
        }
      ],
      "source": [
        "# Optional sanity check\n",
        "#\n",
        "# Compare aggregated metrics computed from the dump files with the metrics returned by\n",
        "# `core.eval.evaluate_model_on_test`, to verify that the dump procedure matches the project's evaluation.\n",
        "\n",
        "from core.eval import evaluate_model_on_test\n",
        "\n",
        "\n",
        "def agg_dump_metrics(parquet_path: Path):\n",
        "    df = pd.read_parquet(parquet_path)\n",
        "    mean_mae = df[\"mae\"].mean()\n",
        "    mean_mql = df[\"mql\"].mean()\n",
        "    return mean_mql, mean_mae\n",
        "\n",
        "\n",
        "print(\"--- Sanity check: GLOBAL ---\")\n",
        "res_base = evaluate_model_on_test(baseline, df_eval_clean, context_length=ctx_g, n_samples=n_g)\n",
        "res_sft  = evaluate_model_on_test(sft_general, df_eval_clean, context_length=ctx_g, n_samples=n_g)\n",
        "\n",
        "dump_base = agg_dump_metrics(global_baseline_path)\n",
        "dump_sft  = agg_dump_metrics(global_sftgen_path)\n",
        "\n",
        "print(\"baseline (core): MQL=\", res_base[\"mean_quantile_loss\"], \" MAE=\", res_base[\"mean_mae\"])\n",
        "print(\"baseline (dump): MQL=\", dump_base[0], \" MAE=\", dump_base[1])\n",
        "print(\"sft_general (core): MQL=\", res_sft[\"mean_quantile_loss\"], \" MAE=\", res_sft[\"mean_mae\"])\n",
        "print(\"sft_general (dump): MQL=\", dump_sft[0], \" MAE=\", dump_sft[1])\n",
        "\n",
        "print(\"Abs. diff baseline (dump-core):\", abs(dump_base[0] - res_base[\"mean_quantile_loss\"]), abs(dump_base[1] - res_base[\"mean_mae\"]))\n",
        "print(\"Abs. diff sft_general (dump-core):\", abs(dump_sft[0] - res_sft[\"mean_quantile_loss\"]), abs(dump_sft[1] - res_sft[\"mean_mae\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1aa643bd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 140049,
          "status": "ok",
          "timestamp": 1770901831415,
          "user": {
            "displayName": "Lorenzo Ferrandi",
            "userId": "04697785781964806390"
          },
          "user_tz": -60
        },
        "id": "1aa643bd",
        "outputId": "4427f353-5f58-41bb-bfe4-3d9e58df9705"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n_categories: 11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "baseline | Information Technology: 100%|██████████| 100/100 [00:02<00:00, 44.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\information_technology__baseline.parquet | rows=1700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_general | Information Technology: 100%|██████████| 100/100 [00:02<00:00, 42.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\information_technology__sft_general_ctx_global.parquet | rows=1700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_general | Information Technology: 100%|██████████| 100/100 [00:02<00:00, 43.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\information_technology__sft_general_ctx_cat.parquet | rows=1700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_category | Information Technology: 100%|██████████| 100/100 [00:02<00:00, 46.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\information_technology__sft_category.parquet | rows=1700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "baseline | Health Care: 100%|██████████| 50/50 [00:01<00:00, 44.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\health_care__baseline.parquet | rows=900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_general | Health Care: 100%|██████████| 50/50 [00:01<00:00, 44.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\health_care__sft_general_ctx_global.parquet | rows=900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_general | Health Care: 100%|██████████| 50/50 [00:01<00:00, 44.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\health_care__sft_general_ctx_cat.parquet | rows=900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_category | Health Care: 100%|██████████| 50/50 [00:01<00:00, 40.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\health_care__sft_category.parquet | rows=900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "baseline | Financials: 100%|██████████| 100/100 [00:02<00:00, 44.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\financials__baseline.parquet | rows=1700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_general | Financials: 100%|██████████| 100/100 [00:02<00:00, 44.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\financials__sft_general_ctx_global.parquet | rows=1700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_general | Financials: 100%|██████████| 100/100 [00:02<00:00, 45.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\financials__sft_general_ctx_cat.parquet | rows=1700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_category | Financials: 100%|██████████| 100/100 [00:02<00:00, 41.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\financials__sft_category.parquet | rows=1700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "baseline | Consumer Discretionary: 100%|██████████| 200/200 [00:04<00:00, 42.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\consumer_discretionary__baseline.parquet | rows=2000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_general | Consumer Discretionary: 100%|██████████| 200/200 [00:04<00:00, 41.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\consumer_discretionary__sft_general_ctx_global.parquet | rows=2000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_general | Consumer Discretionary: 100%|██████████| 200/200 [00:04<00:00, 43.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\consumer_discretionary__sft_general_ctx_cat.parquet | rows=2000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_category | Consumer Discretionary: 100%|██████████| 200/200 [00:04<00:00, 45.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\consumer_discretionary__sft_category.parquet | rows=2000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "baseline | Consumer Staples: 100%|██████████| 100/100 [00:02<00:00, 40.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\consumer_staples__baseline.parquet | rows=1100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_general | Consumer Staples: 100%|██████████| 100/100 [00:02<00:00, 41.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\consumer_staples__sft_general_ctx_global.parquet | rows=1100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_general | Consumer Staples: 100%|██████████| 100/100 [00:02<00:00, 43.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\consumer_staples__sft_general_ctx_cat.parquet | rows=1100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_category | Consumer Staples: 100%|██████████| 100/100 [00:02<00:00, 40.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\consumer_staples__sft_category.parquet | rows=1100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "baseline | Industrials: 100%|██████████| 200/200 [00:04<00:00, 42.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\industrials__baseline.parquet | rows=3600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_general | Industrials: 100%|██████████| 200/200 [00:04<00:00, 43.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\industrials__sft_general_ctx_global.parquet | rows=3600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_general | Industrials: 100%|██████████| 200/200 [00:04<00:00, 43.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\industrials__sft_general_ctx_cat.parquet | rows=3600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_category | Industrials: 100%|██████████| 200/200 [00:05<00:00, 39.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\industrials__sft_category.parquet | rows=3600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "baseline | Energy: 100%|██████████| 50/50 [00:01<00:00, 38.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\energy__baseline.parquet | rows=300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_general | Energy: 100%|██████████| 50/50 [00:01<00:00, 40.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\energy__sft_general_ctx_global.parquet | rows=300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_general | Energy: 100%|██████████| 50/50 [00:01<00:00, 43.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\energy__sft_general_ctx_cat.parquet | rows=300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_category | Energy: 100%|██████████| 50/50 [00:01<00:00, 44.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\energy__sft_category.parquet | rows=300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "baseline | Communication Services: 100%|██████████| 50/50 [00:01<00:00, 41.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\communication_services__baseline.parquet | rows=300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_general | Communication Services: 100%|██████████| 50/50 [00:01<00:00, 42.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\communication_services__sft_general_ctx_global.parquet | rows=300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_general | Communication Services: 100%|██████████| 50/50 [00:01<00:00, 42.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\communication_services__sft_general_ctx_cat.parquet | rows=300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_category | Communication Services: 100%|██████████| 50/50 [00:01<00:00, 42.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\communication_services__sft_category.parquet | rows=300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "baseline | Materials: 100%|██████████| 200/200 [00:04<00:00, 41.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\materials__baseline.parquet | rows=400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_general | Materials: 100%|██████████| 200/200 [00:04<00:00, 40.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\materials__sft_general_ctx_global.parquet | rows=400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_general | Materials: 100%|██████████| 200/200 [00:04<00:00, 44.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\materials__sft_general_ctx_cat.parquet | rows=400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_category | Materials: 100%|██████████| 200/200 [00:04<00:00, 44.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\materials__sft_category.parquet | rows=400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "baseline | Real Estate: 100%|██████████| 150/150 [00:03<00:00, 42.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\real_estate__baseline.parquet | rows=300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_general | Real Estate: 100%|██████████| 150/150 [00:03<00:00, 41.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\real_estate__sft_general_ctx_global.parquet | rows=300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_general | Real Estate: 100%|██████████| 150/150 [00:03<00:00, 42.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\real_estate__sft_general_ctx_cat.parquet | rows=300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sft_category | Real Estate: 100%|██████████| 150/150 [00:03<00:00, 43.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: outputs\\eval_dumps\\sft\\real_estate__sft_category.parquet | rows=300\n",
            "[Utilities] skip: no tickers available in eval split\n",
            "Done. DUMPS_DIR: outputs\\eval_dumps\\sft\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Industry/category evaluation dumps\n",
        "#\n",
        "# For each GICS Level-1 industry group we create four dump files:\n",
        "#   1) baseline evaluated with the category context length (ctx=cat)\n",
        "#   2) global SFT evaluated with the *global* context length (ctx=global)  [matches the original report setting]\n",
        "#   3) global SFT evaluated with the category context length (ctx=cat)     [fair paired comparison vs category model]\n",
        "#   4) category SFT evaluated with the category context length (ctx=cat)\n",
        "\n",
        "categories = list(GICS_LEVEL_1.keys())\n",
        "print(\"n_categories:\", len(categories))\n",
        "\n",
        "for cat in categories:\n",
        "    tickers = [t for t in GICS_LEVEL_1[cat] if t in df_eval_clean.columns]\n",
        "    if len(tickers) == 0:\n",
        "        print(f\"[{cat}] skip: no tickers available in eval split\")\n",
        "        continue\n",
        "\n",
        "    cat_hp = get_hparams(cat) if cat in hp_by_group else None\n",
        "    if cat_hp is None:\n",
        "        print(f\"[{cat}] skip: no tuned hyperparameters found for this group\")\n",
        "        continue\n",
        "\n",
        "    df_cat = df_eval_clean[tickers]\n",
        "    ctx_cat = cat_hp[\"context_length\"]\n",
        "    n_cat = cat_hp[\"n_eval_samples\"]\n",
        "    starts_cat = sample_start_indices(len(df_cat), ctx_cat, n_cat, seed=42)\n",
        "\n",
        "    # Load category checkpoint\n",
        "    ckpt = category_ckpt(cat)\n",
        "    if not ckpt.exists():\n",
        "        print(f\"[{cat}] checkpoint not found: {ckpt} (skipping this category)\")\n",
        "        continue\n",
        "\n",
        "    sft_cat = Chronos2Pipeline.from_pretrained(\n",
        "        str(ckpt),\n",
        "        device_map=DEVICE,\n",
        "        dtype=torch.float32,\n",
        "    )\n",
        "\n",
        "    cat_slug = slugify(cat)\n",
        "    out_base = DUMPS_DIR / f\"{cat_slug}__baseline.parquet\"\n",
        "    out_gen_report = DUMPS_DIR / f\"{cat_slug}__sft_general_ctx_global.parquet\"\n",
        "    out_gen_fair = DUMPS_DIR / f\"{cat_slug}__sft_general_ctx_cat.parquet\"\n",
        "    out_cat = DUMPS_DIR / f\"{cat_slug}__sft_category.parquet\"\n",
        "\n",
        "    # 1) Baseline (ctx=cat)\n",
        "    _ = eval_dump_windows(baseline, df_cat, ctx_cat, starts_cat, \"baseline\", cat, out_base)\n",
        "\n",
        "    # 2) Global SFT, global context length (ctx=global)\n",
        "    ctx_global = global_hp[\"context_length\"]\n",
        "    starts_global_ctx = sample_start_indices(len(df_cat), ctx_global, n_cat, seed=42)\n",
        "    _ = eval_dump_windows(sft_general, df_cat, ctx_global, starts_global_ctx, \"sft_general\", cat, out_gen_report)\n",
        "\n",
        "    # 3) Global SFT, category context length (ctx=cat)\n",
        "    _ = eval_dump_windows(sft_general, df_cat, ctx_cat, starts_cat, \"sft_general\", cat, out_gen_fair)\n",
        "\n",
        "    # 4) Category SFT (ctx=cat)\n",
        "    _ = eval_dump_windows(sft_cat, df_cat, ctx_cat, starts_cat, \"sft_category\", cat, out_cat)\n",
        "\n",
        "    # Free GPU memory between categories\n",
        "    del sft_cat\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Done. DUMPS_DIR:\", DUMPS_DIR)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "H100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "chronos_dnlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
