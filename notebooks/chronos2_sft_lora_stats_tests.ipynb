{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a910a7b2",
   "metadata": {
    "id": "a910a7b2"
   },
   "source": [
    "# Chronos-2 SFT+Lora — Statistical Significance Tests (Global vs Industry-Specific)\n",
    "\n",
    "This notebook loads the evaluation dumps generated by `chronos2_sft_lora_eval_dump.ipynb` and tests whether performance differences between models are statistically robust.\n",
    "\n",
    "We treat each **ticker** as one statistical unit (paired setting):\n",
    "1. Aggregate metrics per ticker (averaged over sampled evaluation windows).\n",
    "2. Compute paired deltas between models for the same tickers.\n",
    "3. Run a Wilcoxon signed-rank test and a bootstrap 95% confidence interval for the mean delta.\n",
    "4. For sector-level results (multiple sectors), apply Benjamini–Hochberg (FDR) correction.\n",
    "\n",
    "**Sign convention:** Δ = metric(model B) − metric(model A). For MAE/MQL, lower is better, so **negative Δ means model B improves over model A**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7b4513",
   "metadata": {
    "id": "cd7b4513"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "# Repository root (to keep paths consistent across machines)\n",
    "current_dir = os.getcwd()\n",
    "project_root = os.path.dirname(current_dir)\n",
    "sys.path.append(project_root)\n",
    "\n",
    "\n",
    "REPO_ROOT = Path(os.getcwd()).parent\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "# Support both layouts (repo root vs notebooks working dir)\n",
    "cand1 = REPO_ROOT / \"notebooks\" / \"outputs\"\n",
    "cand2 = REPO_ROOT / \"outputs\"\n",
    "OUTPUTS_BASE = cand1 if cand1.exists() else cand2\n",
    "\n",
    "DUMPS_DIR = OUTPUTS_BASE / \"eval_dumps\" / \"sft_lora\"\n",
    "if not DUMPS_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Dump directory not found: {DUMPS_DIR}\")\n",
    "\n",
    "OUT_DIR = OUTPUTS_BASE / \"stats_results\" / \"sft_lora\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "print(\"DUMPS_DIR:\", DUMPS_DIR)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a47c3a",
   "metadata": {
    "id": "f6a47c3a"
   },
   "outputs": [],
   "source": [
    "# Load all Parquet dumps\n",
    "\n",
    "paths = sorted(DUMPS_DIR.glob(\"*.parquet\"))\n",
    "print(\"n_parquet:\", len(paths))\n",
    "if len(paths) == 0:\n",
    "    raise RuntimeError(\"No parquet files found in DUMPS_DIR\")\n",
    "\n",
    "dfs = []\n",
    "for p in paths:\n",
    "    df = pd.read_parquet(p)\n",
    "    df[\"source_file\"] = p.name\n",
    "    dfs.append(df)\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "print(\"df_all:\", df_all.shape)\n",
    "df_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863d7e66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1770968321564,
     "user": {
      "displayName": "Lorenzo Ferrandi",
      "userId": "04697785781964806390"
     },
     "user_tz": -60
    },
    "id": "863d7e66",
    "outputId": "306ef20a-eed0-420a-b05f-b9a1f4d73b22"
   },
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2) Aggregate per ticker (unit of analysis)\n",
    "# -----------------------------\n",
    "metrics = [\"mae\", \"mql\"]\n",
    "agg = (\n",
    "    df_all\n",
    "    .groupby([\"group\", \"model\", \"ticker\"], as_index=False)[metrics]\n",
    "    .mean()\n",
    ")\n",
    "print(\"agg:\", agg.shape)\n",
    "agg.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9edbb4",
   "metadata": {
    "id": "aa9edbb4"
   },
   "outputs": [],
   "source": [
    "# Statistical helpers\n",
    "\n",
    "def bootstrap_mean_ci(delta: np.ndarray, n_boot: int = 2000, ci: float = 0.95, seed: int = 123):\n",
    "    \"\"\"Bootstrap CI for the mean of delta.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = len(delta)\n",
    "    if n == 0:\n",
    "        return (np.nan, np.nan)\n",
    "    boots = []\n",
    "    for _ in range(n_boot):\n",
    "        sample = rng.choice(delta, size=n, replace=True)\n",
    "        boots.append(sample.mean())\n",
    "    boots = np.array(boots)\n",
    "    lo = np.quantile(boots, (1-ci)/2)\n",
    "    hi = np.quantile(boots, 1-(1-ci)/2)\n",
    "    return float(lo), float(hi)\n",
    "\n",
    "def bh_fdr(pvals: np.ndarray):\n",
    "    \"\"\"Benjamini–Hochberg FDR correction.\"\"\"\n",
    "    pvals = np.asarray(pvals, dtype=float)\n",
    "    n = len(pvals)\n",
    "    order = np.argsort(pvals)\n",
    "    ranked = pvals[order]\n",
    "    adj = np.empty(n, dtype=float)\n",
    "    prev = 1.0\n",
    "    for i in range(n-1, -1, -1):\n",
    "        rank = i+1\n",
    "        val = ranked[i] * n / rank\n",
    "        prev = min(prev, val)\n",
    "        adj[i] = prev\n",
    "    out = np.empty(n, dtype=float)\n",
    "    out[order] = np.clip(adj, 0, 1)\n",
    "    return out\n",
    "\n",
    "def paired_test_group(agg_df: pd.DataFrame, group: str, model_a: str, model_b: str, metric: str):\n",
    "    \"\"\"Paired test on ticker-level metric averages (delta = B - A).\"\"\"\n",
    "    a = agg_df[(agg_df[\"group\"] == group) & (agg_df[\"model\"] == model_a)][[\"ticker\", metric]]\n",
    "    b = agg_df[(agg_df[\"group\"] == group) & (agg_df[\"model\"] == model_b)][[\"ticker\", metric]]\n",
    "\n",
    "    m = a.merge(b, on=\"ticker\", suffixes=(\"_a\", \"_b\"))\n",
    "    if len(m) < 2:\n",
    "        return None\n",
    "\n",
    "    delta = (m[f\"{metric}_b\"] - m[f\"{metric}_a\"]).to_numpy()\n",
    "    mean_d = float(np.mean(delta))\n",
    "    med_d = float(np.median(delta))\n",
    "    ci_lo, ci_hi = bootstrap_mean_ci(delta, n_boot=5000, ci=0.95, seed=123)\n",
    "\n",
    "    # Wilcoxon signed-rank test (paired, non-parametric)\n",
    "    try:\n",
    "        p = float(wilcoxon(delta).pvalue)\n",
    "    except Exception:\n",
    "        p = np.nan\n",
    "\n",
    "    return {\n",
    "        \"group\": group,\n",
    "        \"metric\": metric,\n",
    "        \"model_a\": model_a,\n",
    "        \"model_b\": model_b,\n",
    "        \"n_tickers\": int(len(m)),\n",
    "        \"mean_delta_b_minus_a\": mean_d,\n",
    "        \"median_delta_b_minus_a\": med_d,\n",
    "        \"ci95_lo\": ci_lo,\n",
    "        \"ci95_hi\": ci_hi,\n",
    "        \"p_wilcoxon\": p,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac23707",
   "metadata": {
    "id": "5ac23707"
   },
   "outputs": [],
   "source": [
    "# Build comparisons\n",
    "\n",
    "comparisons = []\n",
    "\n",
    "# GLOBAL: baseline vs LoRA general\n",
    "for metric in metrics:\n",
    "    r = paired_test_group(agg, \"global\", \"baseline\", \"lora_general\", metric)\n",
    "    if r:\n",
    "        comparisons.append(r)\n",
    "\n",
    "# CATEGORY-LEVEL tests:\n",
    "# We use the FAIR subset for categories:\n",
    "#  - baseline:            __baseline.parquet\n",
    "#  - lora_general (fair): __lora_general_ctx_cat.parquet\n",
    "#  - lora_category:       __lora_category.parquet\n",
    "cat_mask_general_fair = df_all[\"source_file\"].str.contains(\"__lora_general_ctx_cat.parquet\")\n",
    "cat_mask_category     = df_all[\"source_file\"].str.contains(\"__lora_category.parquet\")\n",
    "cat_mask_baseline     = df_all[\"source_file\"].str.contains(\"__baseline.parquet\") & (df_all[\"group\"] != \"global\")\n",
    "\n",
    "df_cat_fair = df_all[cat_mask_general_fair | cat_mask_category | cat_mask_baseline].copy()\n",
    "\n",
    "agg_cat_fair = (\n",
    "    df_cat_fair\n",
    "    .groupby([\"group\", \"model\", \"ticker\"], as_index=False)[metrics]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "cats = sorted([g for g in agg_cat_fair[\"group\"].unique() if g != \"global\"])\n",
    "print(\"n_categories in dumps:\", len(cats))\n",
    "\n",
    "for cat in cats:\n",
    "    for metric in metrics:\n",
    "        r1 = paired_test_group(agg_cat_fair, cat, \"baseline\", \"lora_category\", metric)\n",
    "        if r1: comparisons.append(r1)\n",
    "\n",
    "        r2 = paired_test_group(agg_cat_fair, cat, \"lora_general\", \"lora_category\", metric)\n",
    "        if r2: comparisons.append(r2)\n",
    "\n",
    "df_comp = pd.DataFrame(comparisons)\n",
    "df_comp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1682d794",
   "metadata": {
    "id": "1682d794"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Multiple-testing correction (BH/FDR) for category comparisons\n",
    "\n",
    "df_comp[\"p_adj_bh\"] = np.nan\n",
    "\n",
    "for (metric, model_a, model_b), sub in df_comp[df_comp[\"group\"] != \"global\"].groupby([\"metric\", \"model_a\", \"model_b\"]):\n",
    "    p = sub[\"p_wilcoxon\"].to_numpy()\n",
    "    df_comp.loc[sub.index, \"p_adj_bh\"] = bh_fdr(p)\n",
    "\n",
    "df_comp_sorted = df_comp.sort_values([\"metric\", \"model_a\", \"model_b\", \"p_adj_bh\", \"p_wilcoxon\"])\n",
    "df_comp_sorted.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c34ef89",
   "metadata": {
    "id": "4c34ef89"
   },
   "outputs": [],
   "source": [
    "# Save results\n",
    "\n",
    "out_csv = OUT_DIR / \"paired_tests_ticker_level.csv\"\n",
    "df_comp_sorted.to_csv(out_csv, index=False)\n",
    "print(\"Saved:\", out_csv)\n",
    "\n",
    "# Quick view: global results + category results for (baseline -> lora_category) on MQL\n",
    "display(df_comp_sorted[df_comp_sorted[\"group\"] == \"global\"])\n",
    "display(df_comp_sorted[\n",
    "    (df_comp_sorted[\"group\"] != \"global\") &\n",
    "    (df_comp_sorted[\"metric\"] == \"mql\") &\n",
    "    (df_comp_sorted[\"model_a\"] == \"baseline\") &\n",
    "    (df_comp_sorted[\"model_b\"] == \"lora_category\")\n",
    "].sort_values(\"p_adj_bh\"))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "H100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "chronos_dnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
